# -*- coding: utf-8 -*-
"""implement_resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fBICiGyLVbOuQW0Q-r9kC_F2c1vedE6a

# Notebook Set-Up

## Install Dependencies
"""

import torchvision.transforms as transforms
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
from torchvision import *
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable

import numpy as np
import random
#import matplotlib.pyplot as plt
import time
import copy
import os
import csv
import time

"""## Define Functions: Set Device (GPU | CPU) and Seed"""

# def set_device():
#   '''Set device to use GPU if possible, else use CPU'''
#   device = "cuda" if torch.cuda.is_available() else "cpu"
#   if device != "cuda":
#     print("WARNING: For this notebook to perform best, "
#         "if possible, in the menu under `Runtime` -> "
#         "`Change runtime type.`  select `GPU` ")
#   else:
#     print("GPU is enabled in this notebook.")

#   return device

# def set_seed(seed=None, seed_torch=True):
#   if seed is None:
#     seed = np.random.choice(2 ** 32)
#   random.seed(seed)
#   np.random.seed(seed)
#   if seed_torch:
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
#     torch.cuda.manual_seed(seed)
#     torch.backends.cudnn.benchmark = False
#     torch.backends.cudnn.deterministic = True

#   print(f'Random seed {seed} has been set.')

# def seed_worker(worker_id):
#   worker_seed = torch.initial_seed() % 2**32
#   np.random.seed(worker_seed)
#   random.seed(worker_seed)

"""## Set Device and Seed"""

# device = set_device()
# set_seed(seed=2022)

"""## Mount Google Drive"""

#from google.colab import drive
#drive.mount('/content/drive')

"""## Set Directories"""

result_dir = '../results/experiment1/run2/'

### SOCIAL INTERACTIONS - NO DUPLICATES DIRECTORIES 

#train_dir = './experiment1_train_nma'
#val_dir = './experiment1_valid_nma'
#test_dir = './experiment1_test_nma'

### SOCIAL INTERACTIONS - DUPLICATE DIRECTORIES 

#train_dir = './social_train_nma_duplicates'
#val_dir = './social_valid_nma_duplicates'
#test_dir = './social_test_nma_duplicates'

### POSTURE - DUPLICATE DIRECTORIES 

train_dir = './train_motion_NOduplicates'
val_dir = './valid_motion_NOduplicates'
test_dir = './test_motion_NOduplicates'

### MOTION - DUPLICATE DIRECTORIES 

#train_dir = './train_motion_duplicates'
#val_dir = './valid_motion_duplicates'
#test_dir = './test_motion_duplicates'

if not os.path.exists(result_dir):
  os.makedirs(result_dir)

"""# Set Hyperperameters"""
outModelName = 'experiment1_motion_NOduplicates_batchsize64_lre-3_dropout0.1_epochs200' # CHANGE THIS TO SET-UP NEW RESULTS AND CHECKPOINTS FILENAMES
#outModelName = 'experiment1_social-interactions_duplicates_batchsize64_lre-3_NOdropout_epochs400' # CHANGE THIS TO SET-UP NEW RESULTS AND CHECKPOINTS FILENAMES
batch_size = 64
base_learning_rate = 1e-3
best_accuracy = 0
start_epoch = 0  # start from epoch 0 or last checkpoint epoch
max_epoch = 200
droprate = .1
#use_cuda = torch.cuda.is_available()
freeze_layers = False # freeze all but the output layer (fc)

classes = np.unique(np.array(os.listdir(train_dir) + os.listdir(test_dir)))
classes = classes[classes != '.DS_Store']
num_classes = len(classes)
print('num_classes =',num_classes,':',classes)

"""# Load Data"""

# Transform Properties
data_transforms = transforms.Compose([
            transforms.Resize((224,224)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.ColorJitter(brightness=0.5, hue=0.3),
            transforms.RandomGrayscale(p=0.5),
            transforms.RandomRotation(degrees=5),
            transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])

# Load Data
train_dataset = datasets.ImageFolder(train_dir,data_transforms)
val_dataset = datasets.ImageFolder(val_dir, data_transforms)
test_dataset = datasets.ImageFolder(test_dir,data_transforms)
print(train_dataset,'\n')
print(val_dataset,'\n')
print(test_dataset,'\n')

# Data Loaders
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) # DONT NEED TO BATCH THE TEST SET

# Check Train and Test Data Properties
train_images, train_labels = next(iter(train_dataloader))
print('Properties of Training Set Batch')
print('   # of images:',train_images.shape[0])
print('   # of labels:', len(train_labels.unique()),'of', num_classes,'\n')

val_images, val_labels = next(iter(val_dataloader)) 
print('Properties of Validation Set Batch')
print('   # of images:',val_images.shape[0])
print('   # of labels:', len(val_labels.unique()),'of', num_classes,'\n')

test_images, test_labels = next(iter(test_dataloader)) 
print('Properties of Testing Set Batch')
print('   # of images:',test_images.shape[0])
print('   # of labels:', len(test_labels.unique()),'of', num_classes)

"""## Visulize Image Inputs"""

def imshow(images, labels):
  '''Plot Batch Images as a grid'''

  # Transform images to a grid
  num_columns = min([16, int(images.shape[0]/4)]) # <=16 columns in plot
  images_grid = torchvision.utils.make_grid(images,nrow=num_columns)
  
  # Re-order Grid Dimensions
  images_grid = images_grid.numpy().transpose((1, 2, 0))
    
  # Normalize Image Pixel Values
  mean = np.array([0.485, 0.456, 0.406])
  std = np.array([0.229, 0.224, 0.225])
  images_normalized = np.clip(std * images_grid + mean, 0, 1)

  # Reshape labels
  labels = np.array(labels)
  num_rows = int(np.ceil(len(labels)/num_columns))
  num_padding = int(num_rows * num_columns - len(labels)) # use if batch size is incompatible for reshape
  labels_padded = np.append(labels, ['' for x in range(num_padding)])
  labels_grid = labels_padded.reshape(num_rows,num_columns)
  print(labels_grid)

# Use pytorch models using pre-trained weights
net = models.resnet50(pretrained=True) # DEPRICATE

def append_dropout(model, rate=droprate):
  for name, module in model.named_children():
    if len(list(module.children())) > 0:
      append_dropout(module)
    if isinstance(module, nn.Conv2d):
      new = nn.Sequential(module, nn.Dropout2d(p=rate, inplace=True))
      setattr(model, name, new)

# Add dropout layers after each Conv2d
append_dropout(net)
#net.avgpool = nn.Sequential(nn.Dropout2d(p=droprate,inplace=True),nn.AdaptiveAvgPool2d(output_size=(1, 1)))

# Re-define output layer with correct number of classes
net.fc = nn.Linear(net.fc.in_features,num_classes)

"""## Define Loss and Optimization Functions"""
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=base_learning_rate, momentum=0.9, weight_decay=1e-4)

"""## Define Training and Test Functions"""

# Commented out IPython magic to ensure Python compatibility.
def train(net, epoch):
  '''Train the model on the test set'''
  print('\nEpoch: %d' % epoch)
  net.train()
  train_loss = 0
  correct = 0
  total = 0
  for batch_idx, (inputs, targets) in enumerate(train_dataloader):

    optimizer.zero_grad()
    inputs, targets = Variable(inputs), Variable(targets)
    outputs = net(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()

    train_loss += loss.item()
    _, predicted = torch.max(outputs.data, 1)
    total += targets.size(0)
    correct += predicted.eq(targets.data).cpu().sum()

    if batch_idx % 500 == 0:
      print(batch_idx, len(train_dataloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
           % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))
  return (train_loss/batch_idx, 100.*correct/total)


def test(net, epoch, outModelName):
  '''Test the model on the validation set'''
  global best_accuracy
  net.eval()
  test_loss, correct, total = 0, 0, 0
  with torch.no_grad():
    for batch_idx, (inputs, targets) in enumerate(val_dataloader):

      outputs = net(inputs)
      loss = criterion(outputs, targets)

      test_loss += loss.item()
      _, predicted = torch.max(outputs.data, 1)
      total += targets.size(0)
      correct += predicted.eq(targets.data).cpu().sum()

      if batch_idx % 200 == 0:
        print(batch_idx, len(val_dataloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
             % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))

  # Save checkpoint
  accuracy = 100.*correct/total
  if accuracy > best_accuracy:
    best_accuracy = accuracy
    checkpoint(net, accuracy, epoch, outModelName)
  return (test_loss/batch_idx, 100.*correct/total)

"""## Define Checkpoint and Learning Rate Adjustment"""

checkpointName = result_dir + outModelName + '_checkpoint.t7'

def checkpoint(model, acc, epoch, outModelName):
  '''Save check point'''
  print('Saving..')
  state = {
      'state_dict': model.state_dict(),
      'acc': acc,
      'epoch': epoch,
      'rng_state': torch.get_rng_state()
  }
  torch.save(state, checkpointName)

def adjust_learning_rate(optimizer, epoch):
  '''Decrease the learning rate at 100 and 150 epoch'''
  lr = base_learning_rate
  # if epoch <= 9 and lr > 0.1:
  #   # warm-up training for large minibatch
  #   lr = 0.1 + (base_learning_rate - 0.1) * epoch / 10.
  # if epoch >= 100:
  #   lr /= 10
  # if epoch >= 150:
  #   lr /= 10
  for param_group in optimizer.param_groups:
    param_group['lr'] = lr

"""## Freeze all but the output layer"""

for name, param in net.named_parameters():
  if freeze_layers:
    if 'fc' not in name:
      param.requires_grad = False
  else:
    param.requires_grad = True

"""## Train Model"""

logName = result_dir + outModelName + '_result.csv'
startTime = time.time()

if not os.path.exists(logName):
  with open(logName, 'w') as logfile:
      logwriter = csv.writer(logfile, delimiter=',')
      logwriter.writerow(['epoch', 'train_loss', 'train_acc', 'test_loss', 'test_acc','time_elapsed'])

for epoch in range(start_epoch, max_epoch):
  adjust_learning_rate(optimizer, epoch)
  train_loss, train_acc = train(net, epoch)
  test_loss, test_acc = test(net, epoch, outModelName)
  
  with open(logName, 'a') as logfile:
    logwriter = csv.writer(logfile, delimiter=',')
    time_elapsed = time.time() - startTime
    logwriter.writerow([epoch, train_loss, train_acc.item(), test_loss, test_acc.item(), time_elapsed])
  print(f'Epoch: {epoch} | train acc: {train_acc} | test acc: {test_acc} | time: {time_elapsed}')
